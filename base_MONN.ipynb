{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7flDkjpuGyG",
        "outputId": "493f8f2b-0386-4911-ff28-7138ba0dfdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "y--Hp_ZxucGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rdkit"
      ],
      "metadata": {
        "id": "xjqIHhDcvCyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d7653d-e317-4b67-9731-e5f07a2e54ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2022.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit) (7.1.2)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2022.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import random\n",
        "import argparse\n",
        "import pdb\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "###############################################\n",
        "#                                             #\n",
        "#              Dataset Base Class             #\n",
        "#                                             #\n",
        "###############################################\n",
        "\n",
        "\n",
        "def onek_encoding_unk(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def onek_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def featurization(x):\n",
        "\n",
        "    return x\n",
        "\n",
        "def check_exists(path):\n",
        "    return True if os.path.isfile(path) and os.path.getsize(path) > 0 else False\n",
        "\n",
        "def add_index(input_array, ebd_size):\n",
        "    add_idx, temp_arrays = 0, []\n",
        "    for i in range(input_array.shape[0]):\n",
        "        temp_array = input_array[i,:,:]\n",
        "        masking_indices = temp_array.sum(1).nonzero()\n",
        "        temp_array += add_idx\n",
        "        temp_arrays.append(temp_array)\n",
        "        add_idx = masking_indices[0].max()+1\n",
        "    new_array = np.concatenate(temp_arrays, 0)\n",
        "\n",
        "    return new_array.reshape(-1)\n",
        "\n",
        "\n",
        "class DtiDatasetBase(Dataset):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.data_instances, self.meta_instances = [], []\n",
        "\n",
        "        self.analysis_mode = False\n",
        "\n",
        "        # Gathering All Meta-data from DTI Datasets\n",
        "        self.data_path = os.path.join(args.root_path, f'dataset_{args.dataset_version}/')\n",
        "        complex_dataframe, protein_dataframe, ligand_dataframe = [], [], []\n",
        "\n",
        "        for dataset in args.dataset_subsets.split('+'):\n",
        "            complex_path = f'{self.data_path}complex_metadata_{dataset}.csv'\n",
        "            protein_path = f'{self.data_path}protein_metadata_{dataset}.csv'\n",
        "            ligand_path = f'{self.data_path}ligand_metadata_{dataset}.csv'\n",
        "            complex_dataframe.append(pd.read_csv(complex_path, index_col='complex_id'))\n",
        "            protein_dataframe.append(pd.read_csv(protein_path, index_col='protein_id'))\n",
        "            ligand_dataframe.append(pd.read_csv(ligand_path, index_col='ligand_id'))\n",
        "\n",
        "        self.complex_dataframe = pd.concat(complex_dataframe)\n",
        "        self.protein_dataframe = pd.concat(protein_dataframe)\n",
        "        self.ligand_dataframe = pd.concat(ligand_dataframe)\n",
        "\n",
        "        self.complex_dataframe = self.complex_dataframe[self.complex_dataframe['ba_measure']==args.ba_measure]\n",
        "        if not args.inference_mode:\n",
        "            self.complex_dataframe.dropna(subset=['ba_value'], axis=0, inplace=True)\n",
        "\n",
        "        self.complex_indices = self.complex_dataframe.index\n",
        "        if args.debug_mode or args.toy_test:\n",
        "            self.complex_indices = self.complex_dataframe.index[:args.debug_index]\n",
        "\n",
        "        self.kfold_splits = []\n",
        "\n",
        "        # Which Features to Include?\n",
        "        self.protein_features = args.protein_features # 'esm+blosum+onehot'\n",
        "        self.ligand_features = args.ligand_features\n",
        "\n",
        "    def check_ligand(self, ligand_idx):\n",
        "        return\n",
        "\n",
        "    def check_protein(self, protein_idx):\n",
        "        if self.pdf.loc[protein_idx, 'fasta_length'] >= 1000:\n",
        "            raise FastaLengthException(self.pdf.loc[protein_idx, 'fasta_length'])\n",
        "\n",
        "    def check_complex(self, complex_idx):\n",
        "        return\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.analysis_mode:\n",
        "        # if self.args.analysis_model:\n",
        "            return self.data_instances[idx], self.meta_instances[idx]\n",
        "        else:\n",
        "            return self.data_instances[idx]\n",
        "\n",
        "    def make_random_splits(self):\n",
        "        print(\"Making Random Splits\")\n",
        "        kf = KFold(n_splits=5, shuffle=True)\n",
        "        for a, b in kf.split(self.indices):\n",
        "            train_indices, test_indices = a, b\n",
        "            train_indices, valid_indices = train_test_split(train_indices, test_size=0.05)\n",
        "            self.kfold_splits.append((train_indices, valid_indices, test_indices))\n",
        "\n",
        "class FastaLengthException(Exception):\n",
        "    def __init__(self, fasta_length, message=\"fasta length should not exceed 1000\"):\n",
        "        self.fasta_length = fasta_length\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.fasta_length} -> {self.message}'\n",
        "\n",
        "class NoProteinGraphException(Exception):\n",
        "    def __init__(self, protein_idx, message=\"protein graph structure file not available\"):\n",
        "        self.protein_idx = protein_idx\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.protein_idx} -> {self.message}'\n",
        "\n",
        "class NoProteinFeaturesException(Exception):\n",
        "    def __init__(self, protein_idx, message=\"protein advanced features file not available\"):\n",
        "        self.protein_idx = protein_idx\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.protein_idx} -> {self.message}'\n",
        "\n",
        "class NoComplexGraphException(Exception):\n",
        "    def __init__(self, complex_idx, message=\"complex advanced features file not available\"):\n",
        "        self.complex_idx = complex_idx\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.complex_idx} -> {self.message}'\n",
        "\n",
        "\n",
        "###############################################\n",
        "#                                             #\n",
        "#              Collate Functions              #\n",
        "#                                             #\n",
        "###############################################\n",
        "\n",
        "def stack_and_pad(arr_list, max_length=None):\n",
        "    M = max([x.shape[0] for x in arr_list]) if not max_length else max_length\n",
        "    N = max([x.shape[1] for x in arr_list])\n",
        "    T = np.zeros((len(arr_list), M, N))\n",
        "    t = np.zeros((len(arr_list), M))\n",
        "    s = np.zeros((len(arr_list), M, N))\n",
        "\n",
        "    for i, arr in enumerate(arr_list):\n",
        "        # sum of 16 interaction type, one is enough\n",
        "        if len(arr.shape) > 2:\n",
        "            arr = (arr.sum(axis=2) > 0.0).astype(float)\n",
        "        T[i, 0:arr.shape[0], 0:arr.shape[1]] = arr\n",
        "        t[i, 0:arr.shape[0]] = 1 if arr.sum() != 0.0 else 0\n",
        "        s[i, 0:arr.shape[0], 0:arr.shape[1]] = 1 if arr.sum() != 0.0 else 0\n",
        "    return T, t, s\n",
        "\n",
        "def stack_and_pad_2d(arr_list, block='lower_left', max_length=None):\n",
        "    max0 = max([a.shape[0] for a in arr_list]) if not max_length else max_length\n",
        "    max1 = max([a.shape[1] for a in arr_list])\n",
        "    list_shapes = [a.shape for a in arr_list]\n",
        "\n",
        "    final_result = np.zeros((len(arr_list), max0, max1))\n",
        "    final_masks_2d = np.zeros((len(arr_list), max0))\n",
        "    final_masks_3d = np.zeros((len(arr_list), max0, max1))\n",
        "\n",
        "    if block == 'upper_left':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            # sum of 16 interaction type, one is enough\n",
        "            if len(arr_list[i].shape) > 2:\n",
        "                arr_list[i] = (arr_list[i].sum(axis=2) == True).astype(float)\n",
        "            final_result[i, :shape[0], :shape[1]] = arr_list[i]\n",
        "            final_masks_2d[i, :shape[0]] = 1\n",
        "            final_masks_3d[i, :shape[0], :shape[1]] = 1\n",
        "    elif block == 'lower_right':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, max0-shape[0]:, max1-shape[1]:] = arr_list[i]\n",
        "            final_masks_2d[i, max0-shape[0]:] = 1\n",
        "            final_masks_3d[i, max0-shape[0]:, max1-shape[1]:] = 1\n",
        "    elif block == 'lower_left':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, max0-shape[0]:, :shape[1]] = arr_list[i]\n",
        "            final_masks_2d[i, max0-shape[0]:] = 1\n",
        "            final_masks_3d[i, max0-shape[0]:, :shape[1]] = 1\n",
        "    elif block == 'upper_right':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, :shape[0], max1-shape[1]:] = arr_list[i]\n",
        "            final_masks_2d[i, :shape[0]] = 1\n",
        "            final_masks_3d[i, :shape[0], max1-shape[1]:] = 1\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "    return final_result, final_masks_2d, final_masks_3d\n",
        "\n",
        "def stack_and_pad_3d(arr_list, block='lower_left'):\n",
        "    max0 = max([a.shape[0] for a in arr_list])\n",
        "    max1 = max([a.shape[1] for a in arr_list])\n",
        "    max2 = max([a.shape[2] for a in arr_list])\n",
        "    list_shapes = [a.shape for a in arr_list]\n",
        "\n",
        "    final_result = np.zeros((len(arr_list), max0, max1, max2))\n",
        "    final_masks_2d = np.zeros((len(arr_list), max0))\n",
        "    final_masks_3d = np.zeros((len(arr_list), max0, max1))\n",
        "    final_masks_4d = np.zeros((len(arr_list), max0, max1, max2))\n",
        "\n",
        "    if block == 'upper_left':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, :shape[0], :shape[1], :shape[2]] = arr_list[i]\n",
        "            final_masks_2d[i, :shape[0]] = 1\n",
        "            final_masks_3d[i, :shape[0], :shape[1]] = 1\n",
        "            final_masks_4d[i, :shape[0], :shape[1], :] = 1\n",
        "    elif block == 'lower_right':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, max0-shape[0]:, max1-shape[1]:] = arr_list[i]\n",
        "            final_masks_2d[i, max0-shape[0]:] = 1\n",
        "            final_masks_3d[i, max0-shape[0]:, max1-shape[1]:] = 1\n",
        "            final_masks_4d[i, max0-shape[0]:, max1-shape[1]:, :] = 1\n",
        "    elif block == 'lower_left':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, max0-shape[0]:, :shape[1]] = arr_list[i]\n",
        "            final_masks_2d[i, max0-shape[0]:] = 1\n",
        "            final_masks_3d[i, max0-shape[0]:, :shape[1]] = 1\n",
        "            final_masks_4d[i, max0-shape[0]:, :shape[1], :] = 1\n",
        "    elif block == 'upper_right':\n",
        "        for i, shape in enumerate(list_shapes):\n",
        "            final_result[i, :shape[0], max1-shape[1]:] = arr_list[i]\n",
        "            final_masks_2d[i, :shape[0]] = 1\n",
        "            final_masks_3d[i, :shape[0], max1-shape[1]:] = 1\n",
        "            final_masks_4d[i, :shape[0], max1-shape[1]:, :] = 1\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "    return final_result, final_masks_2d, final_masks_3d, final_masks_4d\n",
        "\n",
        "def ds_normalize(input_array):\n",
        "    # Doubly Stochastic Normalization of Edges from CVPR 2019 Paper\n",
        "    assert len(input_array.shape) == 3\n",
        "    input_array = input_array / np.expand_dims(input_array.sum(1)+1e-8, axis=1)\n",
        "    output_array = np.einsum('ijb,jkb->ikb', input_array,\n",
        "                             input_array.transpose(1, 0, 2))\n",
        "    output_array = output_array / (output_array.sum(0)+1e-8)\n",
        "\n",
        "    return output_array"
      ],
      "metadata": {
        "id": "ptULD5SiuJG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_index(input_array, ebd_size):\n",
        "    add_idx, temp_arrays = 0, []\n",
        "    for i in range(input_array.shape[0]): #batch size\n",
        "        temp_array = input_array[i,:,:]\n",
        "        masking_indices = temp_array.sum(1).nonzero()\n",
        "        #print(masking_indices)\n",
        "        temp_array += add_idx\n",
        "        temp_arrays.append(temp_array)\n",
        "        add_idx = masking_indices[0].max()+1\n",
        "    new_array = np.concatenate(temp_arrays, 0)\n",
        "\n",
        "    return new_array.reshape(-1)"
      ],
      "metadata": {
        "id": "-fyb0OBbTUL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOSUM_DICT = {\n",
        "\t'A': [4,0,-2,-1,-2,0,-2,-1,-1,-1,-1,-2,-1,-1,-1,1,0,0,-3,-2],\n",
        "\t'C': [0,9,-3,-4,-2,-3,-3,-1,-3,-1,-1,-3,-3,-3,-3,-1,-1,-1,-2,-2],\n",
        "\t'D': [-2,-3,6,2,-3,-1,-1,-3,-1,-4,-3,1,-1,0,-2,0,-1,-3,-4,-3],\n",
        "\t'E': [-1,-4,2,5,-3,-2,0,-3,1,-3,-2,0,-1,2,0,0,-1,-2,-3,-2],\n",
        "\t'F': [-2,-2,-3,-3,6,-3,-1,0,-3,0,0,-3,-4,-3,-3,-2,-2,-1,1,3],\n",
        "\t'G': [0,-3,-1,-2,-3,6,-2,-4,-2,-4,-3,0,-2,-2,-2,0,-2,-3,-2,-3],\n",
        "\t'H': [-2,-3,-1,0,-1,-2,8,-3,-1,-3,-2,1,-2,0,0,-1,-2,-3,-2,2],\n",
        "\t'I': [-1,-1,-3,-3,0,-4,-3,4,-3,2,1,-3,-3,-3,-3,-2,-1,3,-3,-1],\n",
        "\t'K': [-1,-3,-1,1,-3,-2,-1,-3,5,-2,-1,0,-1,1,2,0,-1,-2,-3,-2],\n",
        "\t'L': [-1,-1,-4,-3,0,-4,-3,2,-2,4,2,-3,-3,-2,-2,-2,-1,1,-2,-1],\n",
        "\t'M': [-1,-1,-3,-2,0,-3,-2,1,-1,2,5,-2,-2,0,-1,-1,-1,1,-1,-1],\n",
        "\t'N': [-2,-3,1,0,-3,0,1,-3,0,-3,-2,6,-2,0,0,1,0,-3,-4,-2],\n",
        "\t'P': [-1,-3,-1,-1,-4,-2,-2,-3,-1,-3,-2,-2,7,-1,-2,-1,-1,-2,-4,-3],\n",
        "\t'Q': [-1,-3,0,2,-3,-2,0,-3,1,-2,0,0,-1,5,1,0,-1,-2,-2,-1],\n",
        "\t'R': [-1,-3,-2,0,-3,-2,0,-3,2,-2,-1,0,-2,1,5,-1,-1,-3,-3,-2],\n",
        "\t'S': [1,-1,0,0,-2,0,-1,-2,0,-2,-1,1,-1,0,-1,4,1,-2,-3,-2],\n",
        "\t'T': [0,-1,-1,-1,-2,-2,-2,-1,-1,-1,-1,0,-1,-1,-1,1,5,0,-2,-2],\n",
        "\t'V': [0,-1,-3,-2,-1,-3,-3,3,-2,1,1,-3,-2,-2,-3,-2,0,4,-3,-1],\n",
        "\t'W': [-3,-2,-4,-3,1,-2,-2,-3,-3,-2,-1,-4,-4,-2,-3,-3,-2,-3,11,2],\n",
        "\t'Y': [-2,-2,-3,-2,3,-3,2,-1,-2,-1,-1,-2,-3,-1,-2,-2,-2,-1,2,7],\n",
        "    'X': [0 for _ in range(20)],\n",
        "\t'unk':[0 for _ in range(20)]}\n",
        "\n",
        "ATOM_LIST = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg',\n",
        "             'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl',\n",
        "             'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H',\n",
        "             'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
        "             'Pt', 'Hg', 'Pb', 'W', 'Ru', 'Nb', 'Re', 'Te', 'Rh', 'Tc',\n",
        "             'Ba', 'Bi', 'Hf', 'Mo', 'U', 'Sm', 'Os', 'Ir', 'Ce','Gd',\n",
        "             'Ga','Cs', 'unk']\n",
        "\n",
        "\n",
        "class DtiDataset(DtiDatasetBase):\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "        self.default_batch_size = 32\n",
        "        max_nb = 10\n",
        "        for complex_idx in tqdm(self.complex_indices):\n",
        "            try:\n",
        "                ligand_idx = self.complex_dataframe.loc[complex_idx, 'ligand_id']\n",
        "                protein_idx = self.complex_dataframe.loc[complex_idx, 'protein_id']\n",
        "                ba_value = self.complex_dataframe.loc[complex_idx, 'ba_value']\n",
        "                self.check_protein(protein_idx)\n",
        "\n",
        "                # Ligand / Atom / Features\n",
        "                atomwise_features = []\n",
        "                smiles = self.ligand_dataframe.loc[ligand_idx, 'smiles']\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                for atom in mol.GetAtoms():\n",
        "                    try: atomwise_features.append(self.atom_features(atom).reshape(1, -1))\n",
        "                    except: atomwise_features.append(np.zeros(82).reshape(1, -1))\n",
        "                atomwise_features = np.vstack(atomwise_features) # atom feature vector, length 82\n",
        "\n",
        "                idxfunc = lambda x: x.GetIdx()\n",
        "                n_atoms = mol.GetNumAtoms()\n",
        "                assert mol.GetNumBonds() >= 0\n",
        "                n_bonds = max(mol.GetNumBonds(), 1)\n",
        "                atom_nb = np.zeros((n_atoms, max_nb), dtype=np.int32)\n",
        "                bond_nb = np.zeros((n_atoms, max_nb), dtype=np.int32)\n",
        "                num_nbs = np.zeros((n_atoms,), dtype=np.int32)\n",
        "                num_nbs_mat = np.zeros((n_atoms, max_nb), dtype=np.int32)\n",
        "\n",
        "                # Ligand / Bond / Features\n",
        "                bondwise_features = ['null' for _ in range(n_bonds)]\n",
        "                for bond in mol.GetBonds():\n",
        "                    a1, a2 = idxfunc(bond.GetBeginAtom()), idxfunc(bond.GetEndAtom())\n",
        "                    bondwise_features[bond.GetIdx()] = self.bond_features(bond).reshape(1, -1)\n",
        "                    # IndexError: index 6 is out of bounds for axis 1 with size 6\n",
        "                    # bond_nb[a1, num_nbs[a1]]=bond.GetIdx()\n",
        "                    atom_nb[a1, num_nbs[a1]] = a2\n",
        "                    atom_nb[a2, num_nbs[a2]] = a1\n",
        "                    bond_nb[a1, num_nbs[a1]] = bond.GetIdx()\n",
        "                    bond_nb[a2, num_nbs[a2]] = bond.GetIdx()\n",
        "                    num_nbs[a1] += 1\n",
        "                    num_nbs[a2] += 1\n",
        "                bondwise_features = np.vstack(bondwise_features)\n",
        "                for i in range(len(num_nbs)):\n",
        "                    num_nbs_mat[i, :num_nbs[i]] = 1\n",
        "\n",
        "                # Protein / Residue / Features\n",
        "                resiwise_features = []\n",
        "                fasta = self.protein_dataframe.loc[protein_idx, 'fasta']\n",
        "                for resi in fasta:\n",
        "                    resiwise_features.append(np.array(self.resi_features(resi)).reshape(1, -1))\n",
        "                resiwise_features = np.vstack(resiwise_features)\n",
        "\n",
        "                # Complex / Residue / 2D Graph\n",
        "                atomatom_graph = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
        "                plip_path = f'{self.data_path}complexes/{complex_idx}/{complex_idx}.plip.npy'\n",
        "                if check_exists(plip_path):\n",
        "                    atomresi_graph = np.load(plip_path)[:,:,:-1]\n",
        "                    atomresi_label = np.ones((atomwise_features.shape[0], resiwise_features.shape[0], 1))\n",
        "                else:\n",
        "                    atomresi_graph = np.zeros((atomwise_features.shape[0], resiwise_features.shape[0], 1))\n",
        "                    atomresi_label = np.zeros((atomwise_features.shape[0], resiwise_features.shape[0], 1))\n",
        "                smiles = ''.join(list(filter(str.isalpha, smiles)))\n",
        "\n",
        "                metadata = (complex_idx, ligand_idx, protein_idx, smiles, fasta, ba_value)\n",
        "                pytrdata = (atomwise_features, bondwise_features, atom_nb, bond_nb, num_nbs_mat, resiwise_features, atomresi_graph, ba_value)\n",
        "\n",
        "                self.data_instances.append(pytrdata)\n",
        "                self.meta_instances.append(metadata)\n",
        "\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "        print(\"Number of data samples for MONN: \", len(self.data_instances))\n",
        "        self.indices = [i for i in range(len(self.data_instances))]\n",
        "\n",
        "    def check_protein(self, protein_idx):\n",
        "        if self.protein_dataframe.loc[protein_idx, 'fasta_length'] >= 1000:\n",
        "            raise FastaLengthException(self.protein_dataframe.loc[protein_idx, 'fasta_length'])\n",
        "\n",
        "    def check_complex(self, complex_idx):\n",
        "        return\n",
        "        # if not check_exists(f'{self.data_path}complexes/{complex_idx}/{complex_idx}.arpeggio.npy'):\n",
        "        #     import pdb;\n",
        "        #     pdb.set_trace()\n",
        "        #     raise NoComplexGraphException(complex_idx)\n",
        "\n",
        "    def atom_features(self, atom):\n",
        "        return np.array(onek_encoding_unk(atom.GetSymbol(), ATOM_LIST)\n",
        "                        + onek_encoding_unk(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n",
        "                        + onek_encoding_unk(atom.GetExplicitValence(), [1, 2, 3, 4, 5, 6])\n",
        "                        + onek_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n",
        "                        + [atom.GetIsAromatic()], dtype=np.float32)\n",
        "\n",
        "    def bond_features(self, bond):\n",
        "        bt = bond.GetBondType()\n",
        "        return np.array([bt == Chem.rdchem.BondType.SINGLE,\n",
        "                        bt == Chem.rdchem.BondType.DOUBLE,\n",
        "                        bt == Chem.rdchem.BondType.TRIPLE,\n",
        "                        bt == Chem.rdchem.BondType.AROMATIC,\n",
        "                        bond.GetIsConjugated(),\n",
        "                        bond.IsInRing()], dtype=np.float32)\n",
        "\n",
        "    def resi_features(self, resi):\n",
        "        return BLOSUM_DICT[resi]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    tensor_list = []\n",
        "    list_atomwise_features = [x[0] for x in batch]\n",
        "    list_bondwise_features = [x[1] for x in batch]\n",
        "    list_atom_neighbors = [x[2] for x in batch]\n",
        "    list_bond_neighbors = [x[3] for x in batch]\n",
        "    list_neighbor_matrices = [x[4] for x in batch]\n",
        "    list_resiwise_features = [x[5] for x in batch]\n",
        "    list_atomresi_graphs = [(x[6] > 0.).astype(np.int_) for x in batch]\n",
        "    list_ba_values = [x[7] for x in batch]\n",
        "\n",
        "    x, y, _ = stack_and_pad(list_atomwise_features)\n",
        "    tensor_list.append(torch.cuda.FloatTensor(x))\n",
        "    tensor_list.append(torch.cuda.FloatTensor(y))\n",
        "    x, _, _ = stack_and_pad(list_bondwise_features)\n",
        "    tensor_list.append(torch.cuda.FloatTensor(x))\n",
        "    x, _, _ = stack_and_pad(list_atom_neighbors)\n",
        "    tensor_list.append(torch.cuda.LongTensor(add_index(x, x.shape[1])))\n",
        "    x, _, _ = stack_and_pad(list_bond_neighbors)\n",
        "    tensor_list.append(torch.cuda.LongTensor(add_index(x, x.shape[1])))\n",
        "    x, _, _ = stack_and_pad(list_neighbor_matrices)\n",
        "    tensor_list.append(torch.cuda.FloatTensor(x))\n",
        "    x, y, _ = stack_and_pad(list_resiwise_features)\n",
        "    tensor_list.append(torch.cuda.FloatTensor(x))\n",
        "    tensor_list.append(torch.cuda.FloatTensor(y))\n",
        "    tensor_list.append(torch.cuda.FloatTensor(list_ba_values).view(-1, 1))\n",
        "    x, _, z = stack_and_pad(list_atomresi_graphs)\n",
        "    tensor_list.append(torch.cuda.FloatTensor(x))\n",
        "    tensor_list.append(torch.cuda.FloatTensor(z))\n",
        "\n",
        "    return tensor_list"
      ],
      "metadata": {
        "id": "6Qfley6aucF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root_path', default = '/content/drive/MyDrive/Colab Notebooks/monn', type =str)\n",
        "parser.add_argument('--dataset_version', default=220722, type=int)\n",
        "parser.add_argument('--dataset_subsets', default='pdb_2020_general', type=str)\n",
        "parser.add_argument('--ba_measure', default='KIKD', type=str)\n",
        "parser.add_argument('--inference_mode', default = False)\n",
        "parser.add_argument('--debug_mode', default=False)\n",
        "parser.add_argument('--toy_test', default=False)\n",
        "parser.add_argument('--debug_index', default = False)\n",
        "parser.add_argument('--protein_features', type = str)\n",
        "parser.add_argument('--ligand_features', type = str)\n",
        "\n",
        "\n",
        "parser.add_argument('--fold', default = 5, type=int)\n",
        "parser.add_argument('--batch_size', default = 8, type=int)\n",
        "#'KIKD' 'IC50'\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "dataset = DtiDataset(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpqjAqG2vg0V",
        "outputId": "9b8c9716-bc68-43ad-f19d-8b26fb2ebace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9136/9136 [00:30<00:00, 296.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data samples for MONN:  9085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Protein(residue) => CNN\n",
        "BLOSUM62 matrix => 각 residue의 특성을 encoding한 matrix, 20x20  \n",
        "각 residue의 max개수를 이용하여 padding  \n",
        "\n",
        "- Compound(atom) => Graph convolution  \n",
        "Node feature : 길이가 82인 feature vector (원자 종류 63 + Degree 6 + Explicit valence 6 + implicit valence 6 + aromatic 1 = 82)  \n",
        "Edge feature(bond) : 길이가 6인 feature vector (결합 종류 4 + conjugate,ring 2 = 6)  \n",
        "\n",
        "tensor list의 길이는 11  \n",
        "1,2 : atomwise feature 각 배치별,\n",
        "3 : bondwise feature  \n",
        "4 : atom neighbors  \n",
        "5 : bond neighbors  \n",
        "6 : neighbor matrix  \n",
        "7,8 : resiwise feature  \n",
        "9 : ba_value  \n",
        "10,11 : atmoresi graph"
      ],
      "metadata": {
        "id": "fy8onOhpnvhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/monn'"
      ],
      "metadata": {
        "id": "A6amQAa78pVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.make_random_splits()\n",
        "indices = dataset.kfold_splits\n",
        "\n",
        "fold_indices = indices[0]\n",
        "train_idx, valid_idx, test_idx = fold_indices[0], fold_indices[1], fold_indices[2]\n",
        "train = SubsetRandomSampler(train_idx)\n",
        "valid = SubsetRandomSampler(valid_idx)\n",
        "test = SubsetRandomSampler(test_idx)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size = 32,  collate_fn = collate_fn, sampler = train)\n",
        "valid_loader = DataLoader(dataset, batch_size =16, collate_fn = collate_fn, sampler = valid)\n",
        "test_loader = DataLoader(dataset, batch_size = 8, collate_fn = collate_fn, sampler = test)\n",
        "\n",
        "\n",
        "data = next(iter(test_loader))\n",
        "#data2 = next(iter(valid_loader))\n",
        "with open(path + '/data/data_sample.pkl', 'wb') as writer:\n",
        "    pickle.dump(data, writer)"
      ],
      "metadata": {
        "id": "kjjb40JBvtLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0a66e8-460a-4331-cdf6-76e01f91a36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making Random Splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i , train_data in enumerate(train_loader):\n",
        "  a = train_data[0]"
      ],
      "metadata": {
        "id": "5klZohR18qYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsCuc2RlP6Zp",
        "outputId": "e4871228-ed4d-42ec-ffa1-9d85d1a6deb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 60, 82])"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 364는 batch size  \n",
        "* Batch의 compound 안에 있는 atom의 최대 개수  \n",
        "* atom feature vector"
      ],
      "metadata": {
        "id": "pVJMtd5OWURA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXMF7y1ulQvu",
        "outputId": "aeee7705-b690-495a-ea59-60b763b25439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1] #masking 정보"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNj-FPy6Yal1",
        "outputId": "fac47323-d5e0-4f6c-a9be-8c3ffd96696a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v_mask = data[1].reshape(data[1].size(0),-1,1)\n",
        "p_mask = data[7].reshape(data[7].size(0),-1,1)"
      ],
      "metadata": {
        "id": "ThgBFki0y11j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = data[0]*v_mask\n",
        "a = torch.sum(a,dim=1)/torch.sum(v_mask, dim = 1)\n",
        "\n",
        "b = data[6]*p_mask\n",
        "b = torch.sum(b,dim=1)/torch.sum(p_mask, dim=1)"
      ],
      "metadata": {
        "id": "m5o2bi7bRVLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape, b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrUoQJJVTgH9",
        "outputId": "5de957fe-44ea-4425-c5f5-3ca02f2d5af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 82]), torch.Size([8, 20]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_softmax(a, mask, dim=-1):\n",
        "\t\ta_max = torch.max(a,dim,keepdim=True)[0]\n",
        "\t\ta_exp = torch.exp(a-a_max)\n",
        "\t\ta_exp = a_exp*mask\n",
        "\t\ta_softmax = a_exp/(torch.sum(a_exp,dim,keepdim=True)+1e-6)\n",
        "\t\treturn a_softmax"
      ],
      "metadata": {
        "id": "vk7-6PMzdwcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc0 = nn.Linear(82, 82).cuda()\n",
        "a = mc0(data[0])"
      ],
      "metadata": {
        "id": "aus0Y9LKzPdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEJo3UKnd_qC",
        "outputId": "40272228-f860-43e1-8013-90162b0aa895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 50, 82])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vs = nn.Linear(82, 1).cuda()\n",
        "a = vs(a)\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTciGejVcdru",
        "outputId": "dade56ec-333f-462a-f4d8-ecf61ce5b664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 50, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "att = mask_softmax(a.view(a.size(0),-1),data[1].view(data[1].size(0),-1))"
      ],
      "metadata": {
        "id": "6jSpLa0sfA79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru = nn.GRUCell(50, 50).cuda()\n",
        "gru(data[1],data[1]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QDVZEbHhRHb",
        "outputId": "82b9d1f1-42ce-453a-a865-d1d236b3477b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PurGmAu8UJRO",
        "outputId": "9d213d75-e3ff-493b-ecf5-63ffc24caf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 41, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 364는 batch size  \n",
        "* Batch의 compound 안에 있는 edge의 최대 개수  \n",
        "* edge feature vector"
      ],
      "metadata": {
        "id": "knNkTsnKm2TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[3].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXJ_rwsyieYt",
        "outputId": "d06bd478-9eab-44dc-d3a2-26a163356493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3040])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- atom neighbors"
      ],
      "metadata": {
        "id": "gbDXllLLm7gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRULdkCblwVj",
        "outputId": "444f48b5-98ad-4165-d428-1b43310c5772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  0,  0,  ..., 14, 14, 14], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atom_features = data[0]\n",
        "bond_features = data[2]\n",
        "atom_neighbor = data[3]\n",
        "bond_neighbor = data[4]\n",
        "\n",
        "batch_size = atom_features.size(0)\n",
        "neighbors = torch.zeros(size=(atom_features.size(0), atom_features.size(1), 10, atom_features.size(2)+6)).cpu()\n",
        "\n",
        "atom_neighbor = atom_neighbor.reshape(batch_size, -1, 10)\n",
        "bond_neighbor = bond_neighbor.reshape(batch_size, -1, 10)\n",
        "\n",
        "atom_id = torch.tensor([atom_neighbor[mol, -1, -1].item() for mol in range(batch_size)]) # atom neighbor에 있는 추가된 index의 값 저장\n",
        "atom_id = atom_id.reshape(-1, 1).unsqueeze(2).expand(batch_size, -1, 10)\n",
        "atom_id = atom_id.cuda()\n",
        "\n",
        "atom_neighbor = (atom_neighbor - atom_id).type(torch.int64) # 추가된 index를 빼서 원래 index로 만들어주기\n",
        "bond_neighbor = (bond_neighbor - atom_id).type(torch.int64) # 추가된 index를 빼서 원래 index로 만들어주기\n",
        "\n",
        "atom_ids = data[1]\n",
        "neighbor_matrix = data[5]\n",
        "for mol_idx, atom_idx in torch.nonzero(atom_ids):\n",
        "\n",
        "    neighbor_idx = torch.nonzero(neighbor_matrix[mol_idx][atom_idx])[-1].item()\n",
        "    atom_indices = atom_neighbor[mol_idx, atom_idx, :neighbor_idx+1]\n",
        "    bond_indices = bond_neighbor[mol_idx, atom_idx, :neighbor_idx+1]\n",
        "\n",
        "    neighbor_atom_tensor = atom_features[mol_idx, atom_indices]\n",
        "    neighbor_bond_tensor = bond_features[mol_idx, bond_indices]\n",
        "\n",
        "    concat_atom_info = torch.cat((neighbor_atom_tensor, neighbor_bond_tensor), dim = 1)\n",
        "    self\n",
        "    neighbors[mol_idx, atom_idx, :atom_indices.size(0), : ] = concat_atom_info\n"
      ],
      "metadata": {
        "id": "C3fX2ExSxvfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def initialize_(batch_size, input_dim, hidden_dim): # batch_size x input_dim x hidden_dim의 weight\n",
        "    weight = torch.nn.Parameter(torch.FloatTensor(input_dim, hidden_dim))\n",
        "    torch.nn.init.kaiming_uniform_(weight)\n",
        "    weight = weight.reshape(1, input_dim, hidden_dim)\n",
        "    weight = weight.expand(batch_size,input_dim, hidden_dim)\n",
        "    return weight\n",
        "def embedding_(input, weight): # batch x 원자 개수 x hidden_dim의 representation\n",
        "    return F.leaky_relu(torch.bmm(input, weight), 0.1)\n",
        "weight1 = initialize_(n.size(0),88,82)"
      ],
      "metadata": {
        "id": "aZET7Q6mYIjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = embedding_(n,weight1).sum(dim = 1).view(8, -1 , 82)\n",
        "updated_local_info = torch.cat((a, a), dim =2)\n",
        "print(updated_local_info.size())\n",
        "\n",
        "weight2 = initialize_(8,164,82)\n",
        "b = embedding_(updated_local_info,weight2)\n",
        "print(b.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfScEqmHYR7p",
        "outputId": "f47254d5-ac94-422a-9807-0b775d683ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 38, 164])\n",
            "torch.Size([8, 38, 82])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "masking 과정  \n",
        "atom - atom neighbor에서 0인데 atom 0과 연결되어 있는 경우  \n",
        "bond - bond neighbor에서 0인데 edge 0인 경우"
      ],
      "metadata": {
        "id": "wOMoBPchChNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- bond neighbors"
      ],
      "metadata": {
        "id": "cKfUFSbqnC_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[5].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY-oO1XeigkM",
        "outputId": "232d14d2-f612-408f-c10c-84b9255cbbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 27, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- neighbor matrix  \n",
        "- 10은 max number(의미?)"
      ],
      "metadata": {
        "id": "qFudPZFLnOHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_layer, hidden_size, kernel_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.hidden_size = hidden_size\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        #self.embed_seq = nn.Embedding(len(self.init_word_features), 20, padding_idx=0)\n",
        "        #self.embed_seq.weight = nn.Parameter(self.init_word_features)\n",
        "        #self.embed_seq.weight.requires_grad = False\n",
        "\n",
        "        self.conv_first = nn.Conv1d(20, self.hidden_size , kernel_size=self.kernel_size,\n",
        "                                    padding=int((self.kernel_size - 1) / 2))\n",
        "        self.conv_last = nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=self.kernel_size,\n",
        "                                   padding=int((self.kernel_size - 1) / 2))\n",
        "\n",
        "        self.plain_CNN = nn.ModuleList([])\n",
        "        for i in range(self.num_layer):\n",
        "            self.plain_CNN.append(nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=self.kernel_size,\n",
        "                                            padding=int((self.kernel_size - 1) / 2)))\n",
        "    def forward(self, x):\n",
        "        sequence = x.transpose(1,2) # batch x h1 x residue\n",
        "        #embedding = self.embed_seq(sequence)\n",
        "        x = F.leaky_relu(self.conv_first(sequence),0.1)\n",
        "        for num in range(self.num_layer):\n",
        "            x = self.plain_CNN[num](x)\n",
        "            x = F.leaky_relu(x,0.1)\n",
        "        x = F.leaky_relu(self.conv_last(x),0.1)\n",
        "        x = x.transpose(1,2)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "67AmynrzSI7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRl5yJ8IS5sa",
        "outputId": "1b426e4d-4240-47d8-d2c3-072e82f6e4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv_first): Conv1d(20, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  (conv_last): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  (plain_CNN): ModuleList(\n",
              "    (0): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (3): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN(4,128,5)\n",
        "output = cnn.cuda()(data[6])"
      ],
      "metadata": {
        "id": "xqhXsPPJSKcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xjgU5LJTzge",
        "outputId": "f7b491b5-b628-4256-8dc9-9570875d4e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 366, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = nn.Embedding(data[6].size(1), 20, padding_idx=0)\n",
        "conv_first = nn.Conv1d(20, 128 , kernel_size=5, padding=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "IXUtENaBO_GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([2,31])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCgMFMIERWml",
        "outputId": "ac178d2d-849d-491b-9169-8b926dc7532e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed(torch.tensor([2,291]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PO11Y54-Qj4N",
        "outputId": "b2e08949-18c2-4ca8-c916-8eaadab4d98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7318,  0.8266, -0.3092,  1.6290, -0.3277,  1.7404, -0.3083,  0.1312,\n",
              "         -0.2667,  0.4664, -0.3234,  0.1287,  0.2820, -0.0963, -1.8444,  0.6796,\n",
              "         -1.2653, -0.3600, -0.3921,  0.6482],\n",
              "        [ 1.1570,  0.3718,  0.0839,  0.3193,  0.7952,  0.4620, -0.9109, -1.0247,\n",
              "         -0.8341,  0.6086,  0.9505, -0.4411, -1.8334, -1.9562, -1.2634,  0.2680,\n",
              "          0.1432, -1.3979, -0.2807,  1.4928]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(1,2,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaHoUeDRQ773",
        "outputId": "41b13226-7546-44d2-828f-c1c7d1a4be46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1544, -0.8639,  1.1462],\n",
              "         [ 0.4451, -0.5022,  0.7280]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_first(data[6].transpose(1,2).cpu()).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVta3OrNP7ZX",
        "outputId": "9c173893-a7ff-4c3b-8150-e74dcf14566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 128, 366])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[6].dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi6wOe5NPkTM",
        "outputId": "8b0990ee-6b18-4220-ec8e-58054d0dec4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Batch의 protein 안에 있는 residue의 최대 개수\n",
        "- Blosum vector 길이 - 20  "
      ],
      "metadata": {
        "id": "ILhziuMUmW81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[7].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSn10QY8ijCE",
        "outputId": "08d8c2a1-3618-466c-f71c-f8039a2b2890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 846])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[8].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrFzQ4NcikEU",
        "outputId": "e94e514d-f6df-4cd3-a997-222cf7aca077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Target value"
      ],
      "metadata": {
        "id": "0-u2va9hjjyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[9].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOzIeWntik8U",
        "outputId": "6aea9b82-0fe8-4c49-dc64-2242f204a1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 27, 846])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDZP2-l3il2s",
        "outputId": "4b28d440-f8d9-4eb1-dd89-f1fc0d9dc1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 27, 846])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 배치별 compound의 atom과 protein의 residue간의 정보"
      ],
      "metadata": {
        "id": "DOuHOJz-no11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "5SP9L2XGugSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''base.py'''\n",
        "@torch.no_grad()\n",
        "def store_representations(self, input, output):\n",
        "    self.representations.append(output.detach().cpu().numpy())\n",
        "    return\n",
        "\n",
        "@torch.no_grad()\n",
        "def store_interactions(self, input, output):\n",
        "    self.interactions.append(output.detach().cpu().numpy())\n",
        "    return\n",
        "\n",
        "@torch.no_grad()\n",
        "def store_atomwise_compound_representations(self, input, output):\n",
        "\tself.atomwise_representations.append(output[-2].detach().cpu().numpy())\n",
        "\tself.compound_representations.append(output[-1].detach().cpu().numpy())\n",
        "\treturn\n",
        "\n",
        "@torch.no_grad()\n",
        "def store_atomwise_resiwise_compound_representations(self, input, output):\n",
        "\tself.atomwise_representations.append(output[-3].detach().cpu().numpy())\n",
        "\tself.resiwise_representations.append(output[-2].detach().cpu().numpy())\n",
        "\tself.compound_representations.append(output[-1].detach().cpu().numpy())\n",
        "\treturn\n",
        "\n",
        "\n",
        "def mask_softmax(a, mask, dim=-1):\n",
        "    a_max = torch.max(a, dim, keepdim=True)[0]\n",
        "    a_exp = torch.exp(a - a_max)\n",
        "    a_exp = a_exp * mask\n",
        "    a_softmax = a_exp / (torch.sum(a_exp, dim, keepdim=True) + 1e-6)\n",
        "    return a_softmax\n",
        "\n",
        "\n",
        "class GraphDenseSequential(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super(GraphDenseSequential, self).__init__(*args)\n",
        "\n",
        "    def forward(self, X, adj, mask):\n",
        "        for module in self._modules.values():\n",
        "            try:\n",
        "                X = module(X, adj, mask)\n",
        "            except BaseException:\n",
        "                X = module(X)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "0_TmSXaDuj96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "atomf_len = 82\n",
        "bondf_len = 6\n",
        "\n",
        "'''\n",
        "- Protein(residue) => CNN\n",
        "BLOSUM62 matrix => 각 residue의 특성을 encoding한 matrix, 20x20\n",
        "각 residue의 max개수를 이용하여 padding\n",
        "\n",
        "- Compound(atom) => Graph convolution\n",
        "Node feature : 길이가 82인 feature vector (원자 종류 63 + Degree 6 + Explicit valence 6 + implicit valence 6 + aromatic 1 = 82)\n",
        "Edge feature(bond) : 길이가 6인 feature vector (결합 종류 4 + conjugate,ring 2 = 6)\n",
        "\n",
        "tensor list의 길이는 11\n",
        "1,2 : atomwise feature 각 배치별,\n",
        "3 : bondwise feature\n",
        "4 : atom neighbors\n",
        "5 : bond neighbors\n",
        "6 : neighbor matrix\n",
        "7,8 : resiwise feature\n",
        "9 : ba_value\n",
        "10,11 : atmoresi graph\n",
        "'''\n",
        "def initialize_(batch_size, input_dim, hidden_dim): # batch_size x input_dim x hidden_dim의 weight\n",
        "    weight = torch.nn.Parameter(torch.FloatTensor(input_dim, hidden_dim)).cuda()\n",
        "    torch.nn.init.kaiming_uniform_(weight)\n",
        "    weight = weight.reshape(1, input_dim, hidden_dim)\n",
        "    weight = weight.expand(batch_size,input_dim, hidden_dim)\n",
        "    return weight\n",
        "\n",
        "def embedding_(input, weight): # batch x 원자 개수 x hidden_dim의 representation\n",
        "    return F.leaky_relu(torch.bmm(input, weight), 0.1)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, GNN_depth=2, k_head=3, hidden_dim1=10, hidden_dim2=10, DAN_depth=2):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.GNN_depth = GNN_depth\n",
        "        self.k_head = k_head\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.hidden_dim2 = hidden_dim2\n",
        "        self.DAN_depth = DAN_depth\n",
        "\n",
        "        self.mpu = MPU(128)\n",
        "        self.warp_gru = Warp_GRU(self.k_head, self.hidden_dim1)\n",
        "\n",
        "        self.cnn = CNN(4, 128, 5)\n",
        "        self.pairwise = Pairwise_pred_module(self.hidden_dim1)\n",
        "        self.affinity = Affinity_pred_module(self.hidden_dim1, self.hidden_dim2, self.DAN_depth)\n",
        "\n",
        "    def forward(self, atom_feature, masking, bond_feature, atom_neighbor, bond_neighbor, neighbor_matrix, residue_feature, residue_mask):\n",
        "        batch_size = atom_feature.size(0)\n",
        "        vertex_initial = atom_feature  # batch x atom개수 x atomf_len\n",
        "\n",
        "        weight_init = initialize_(batch_size, atomf_len, self.hidden_dim1)  # batch x atomf_len x h1\n",
        "        super_weight_init = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)  # batch x h1 x h1\n",
        "\n",
        "        vertex_feature = embedding_(vertex_initial, weight_init)  # batch x atom 개수 x h1\n",
        "        super_node_init = torch.sum(vertex_feature, dim=1, keepdim=True)  # Summation of node features, batch x 1 x h1\n",
        "        super_node_feature = F.tanh(embedding_(super_node_init, super_weight_init))  # super_node_features, batch x 1 x h1\n",
        "        #print('first embedding')\n",
        "        for GNN_iter in range(self.GNN_depth):\n",
        "\n",
        "            u_i = self.mpu(vertex_feature, masking, bond_feature, atom_neighbor, bond_neighbor, neighbor_matrix)\n",
        "            #print('mpu_{}_iter'.format(GNN_iter))\n",
        "            vertex_feature, super_node_feature = self.warp_gru(vertex_feature, super_node_feature, u_i)\n",
        "            #print('warp_gru_{}_iter'.format(GNN_iter))\n",
        "\n",
        "        protein_feature = self.cnn(residue_feature)\n",
        "        #print('cnn')\n",
        "\n",
        "        pairwise_pred = self.pairwise(vertex_feature, protein_feature, masking, residue_mask)\n",
        "        #print('pairwise prediction')\n",
        "        affinity_pred = self.affinity(vertex_feature, super_node_feature, protein_feature, masking, residue_mask, pairwise_pred)\n",
        "\n",
        "        return affinity_pred\n",
        "\n",
        "\n",
        "class MPU(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(MPU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def gather_neighbor_info(self, atom_features,masking, bond_features, atom_neighbor, bond_neighbor, neighbor_matrix):\n",
        "        neighbors = torch.zeros(size=(atom_features.size(0), atom_features.size(1), 10,atom_features.size(2) + 6)).cuda()  # batch x atom x 10 x hidden +6\n",
        "        batch_size = atom_features.size(0)\n",
        "\n",
        "        atom_neighbor = atom_neighbor.reshape(batch_size, -1, 10)\n",
        "        bond_neighbor = bond_neighbor.reshape(batch_size, -1, 10)\n",
        "\n",
        "        atom_id = torch.tensor([atom_neighbor[mol, -1, -1].item() for mol in range(batch_size)])\n",
        "        atom_id = atom_id.reshape(-1, 1).unsqueeze(2).expand(batch_size, -1, 10)\n",
        "        atom_id = atom_id.cuda()\n",
        "\n",
        "        atom_neighbor = (atom_neighbor - atom_id).type(torch.int64)  # add index 하기 전 atom index\n",
        "        bond_neighbor = (bond_neighbor - atom_id).type(torch.int64)  # add index 하기 전 edge index\n",
        "\n",
        "        for mol_idx, atom_idx in torch.nonzero(masking):  # masking된 정보를 이용해 molecule과 atom의 index로 iterate\n",
        "            neighbor_idx = torch.nonzero(neighbor_matrix[mol_idx][atom_idx])[ -1].item()  # 해당 molecule, atom에서 연결 개수\n",
        "            atom_indices = atom_neighbor[mol_idx, atom_idx, :neighbor_idx + 1]  # 연결된 atom의 인덱스\n",
        "            bond_indices = bond_neighbor[mol_idx, atom_idx, :neighbor_idx + 1]  # 연결된 edge의 인덱스\n",
        "\n",
        "            neighbor_atom_tensor = atom_features[mol_idx, atom_indices]\n",
        "            neighbor_bond_tensor = bond_features[mol_idx, bond_indices]\n",
        "\n",
        "            concat_atom_info = torch.cat((neighbor_atom_tensor, neighbor_bond_tensor), dim=1)  # 2 x (hidden + 6)\n",
        "            neighbors[mol_idx, atom_idx, :atom_indices.size(0), :] = concat_atom_info\n",
        "        return neighbors\n",
        "\n",
        "    def forward(self, atom_features, masking, bond_features, atom_neighbor, bond_neighbor, neighbor_matrix):\n",
        "        batch_size = atom_features.size(0)\n",
        "\n",
        "        concatenated_neighbors = self.gather_neighbor_info(atom_features,masking, bond_features, atom_neighbor, bond_neighbor, neighbor_matrix)\n",
        "        concatenated_neighbors = concatenated_neighbors.reshape(-1, 10, concatenated_neighbors.size(-1))  # batch * atom x 10 x hidden +6\n",
        "        local_weight = initialize_(concatenated_neighbors.size(0), self.hidden_dim + 6, self.hidden_dim)\n",
        "        local_embedding = embedding_(concatenated_neighbors, local_weight).sum(dim=1)  # batch * atom x hidden\n",
        "        local = local_embedding.reshape(batch_size, -1, self.hidden_dim)  # batch x atom x hidden\n",
        "\n",
        "        update_local = torch.cat((local, atom_features), dim=2)  # batch x atom x hidden*2\n",
        "        update_local_weight = initialize_(batch_size, self.hidden_dim * 2, self.hidden_dim)\n",
        "        updated_i = embedding_(update_local, update_local_weight)  # batch x atom x hidden\n",
        "        return updated_i\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, batch_size, k_head, hidden_dim1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.k_head = k_head\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.att_weight = initialize_(self.batch_size, self.hidden_dim1, 1)\n",
        "        self.super_att_weight = initialize_(self.batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        self.vertex_att_weight = initialize_(self.batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        self.v_s_weight = initialize_(self.batch_size, self.k_head * self.hidden_dim1, self.hidden_dim1)\n",
        "\n",
        "    def forward(self, vertex, super_node):\n",
        "        v_att = F.tanh(torch.bmm(vertex, self.vertex_att_weight))  # batch x atom 개수 x h1\n",
        "        s_att = F.tanh(torch.bmm(super_node, self.super_att_weight))  # batch x 1 x h1\n",
        "        b_ = torch.mul(v_att, s_att)  # batch x atom 개수 x h1, elementwise multiplication\n",
        "        k_head = []\n",
        "        for i in range(self.k_head):\n",
        "            alpha = F.softmax(torch.bmm(b_, self.att_weight))  # batch x atom 개수 x 1\n",
        "            alpha = alpha.expand(alpha.size(0), alpha.size(1), self.hidden_dim1)  # batch x atom 개수 x h1\n",
        "            alpha_vertex = alpha * vertex  # batch x atom 개수 x h1\n",
        "            k_head.append(alpha_vertex)\n",
        "\n",
        "        concat = torch.stack(k_head, dim=2)  # batch x atom 개수 x k_head x h1\n",
        "        concat = concat.reshape(concat.size(0), concat.size(1),\n",
        "                                concat.size(-1) * concat.size(-2))  # batch x atom 개수 x k_head*h1\n",
        "        output = torch.bmm(concat, self.v_s_weight)  # batch x atom 개수 x h1\n",
        "        output = F.tanh(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Warp_GRU(nn.Module):\n",
        "    def __init__(self, k_head, hidden_dim1):\n",
        "        super(Warp_GRU, self).__init__()\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.k_head = k_head\n",
        "\n",
        "        self.v_gru = nn.GRUCell(self.hidden_dim1, self.hidden_dim1)\n",
        "        self.s_gru = nn.GRUCell(self.hidden_dim1, self.hidden_dim1)\n",
        "\n",
        "    def forward(self, vertex, super_node, u_i):\n",
        "        batch_size = vertex.size(0)\n",
        "\n",
        "        s_weight = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        s_v_weight = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "\n",
        "        weight_11 = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        weight_12 = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        weight_21 = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "        weight_22 = initialize_(batch_size, self.hidden_dim1, self.hidden_dim1)\n",
        "\n",
        "        k_head_attn = Attention(batch_size, self.k_head, self.hidden_dim1)\n",
        "\n",
        "        u_s = F.tanh(torch.bmm(super_node, s_weight))  # batch x 1 x h1\n",
        "        u_s_v = F.tanh(torch.bmm(super_node, s_v_weight))  # batch x 1 x h1, gathering info from super node\n",
        "\n",
        "        u_v_s = k_head_attn(vertex, super_node)  # batch x atom 개수 x h1\n",
        "        g_v_s = torch.sigmoid(torch.bmm(u_v_s, weight_11) + torch.bmm(u_s, weight_12))  # batch x atom 개수 x h1\n",
        "        t_v_s = torch.mul(1 - g_v_s, u_v_s) + torch.mul(g_v_s, u_s)  # batch x atom 개수 x h1\n",
        "\n",
        "        g_s_i = torch.sigmoid(torch.bmm(u_i, weight_21) + torch.bmm(u_s_v, weight_22))  # batch x atom 개수 x h1\n",
        "        t_s_i = torch.mul(1 - g_s_i, u_i) + torch.mul(g_s_i, u_s_v)  # batch x atom 개수 x h1\n",
        "\n",
        "        atom_node_updated = self.v_gru(vertex.view(-1, self.hidden_dim1),t_s_i.view(-1, self.hidden_dim1))  # batch x atom 개수 x h1\n",
        "        atom_node_updated = atom_node_updated.view(batch_size, -1, self.hidden_dim1)\n",
        "        super_node_updated = self.s_gru(super_node.view(-1, self.hidden_dim1),torch.sum(t_v_s, dim=1).view(-1, self.hidden_dim1))  # batch x 1 x h1\n",
        "        super_node_updated = super_node_updated.view(batch_size, -1, self.hidden_dim1)\n",
        "\n",
        "        return atom_node_updated, super_node_updated\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_layer, hidden_size, kernel_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.hidden_size = hidden_size\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # self.embed_seq = nn.Embedding(len(self.init_word_features), 20, padding_idx=0)\n",
        "        # self.embed_seq.weight = nn.Parameter(self.init_word_features)\n",
        "        # self.embed_seq.weight.requires_grad = False\n",
        "\n",
        "        self.conv_first = nn.Conv1d(20, self.hidden_size, kernel_size=self.kernel_size, padding=int((self.kernel_size - 1) / 2))\n",
        "        self.conv_last = nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=self.kernel_size,padding=int((self.kernel_size - 1) / 2))\n",
        "\n",
        "        self.plain_CNN = nn.ModuleList([])\n",
        "        for i in range(self.num_layer):\n",
        "            self.plain_CNN.append(nn.Conv1d(self.hidden_size, self.hidden_size, kernel_size=self.kernel_size,padding=int((self.kernel_size - 1) / 2)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        sequence = x.transpose(1, 2)  # batch x h1 x residue\n",
        "        # embedding = self.embed_seq(sequence)\n",
        "        x = F.leaky_relu(self.conv_first(sequence), 0.1)\n",
        "        #print('cnn first layer')\n",
        "        for num in range(self.num_layer):\n",
        "            x = self.plain_CNN[num](x)\n",
        "            x = F.leaky_relu(x, 0.1)\n",
        "        x = F.leaky_relu(self.conv_last(x), 0.1)\n",
        "        x = x.transpose(1, 2)  # batch x residue x h1\n",
        "        return x\n",
        "\n",
        "\n",
        "class Pairwise_pred_module(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Pairwise_pred_module, self).__init__()\n",
        "        self.hidden_dim = hidden_size\n",
        "\n",
        "    def forward(self,comp_feature, prot_feature, vertex_mask, seq_mask):\n",
        "        batch_size = comp_feature.size(0)\n",
        "        weight_atom = initialize_(batch_size, self.hidden_dim, self.hidden_dim)\n",
        "        weight_residue = initialize_(batch_size, self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "        pairwise_c_feature = embedding_(comp_feature, weight_atom)  # batch x atom x hidden\n",
        "        pairwise_p_feature = embedding_(prot_feature, weight_residue)  # batch x residue x hidden\n",
        "        pairwise_pred = torch.sigmoid(torch.matmul(pairwise_c_feature, pairwise_p_feature.transpose(1, 2)))  # batch x atom x residue\n",
        "        pairwise_mask = torch.matmul(vertex_mask.view(batch_size, -1, 1),seq_mask.view(batch_size, 1, -1))  # batch x atom x residue\n",
        "        pairwise_pred = pairwise_pred * pairwise_mask  # batch x atom x residue\n",
        "\n",
        "        return pairwise_pred\n",
        "\n",
        "\n",
        "class Affinity_pred_module(nn.Module):\n",
        "    def __init__(self, hidden_size, hidden_size2, DAN_depth):\n",
        "        super(Affinity_pred_module, self).__init__()\n",
        "        self.hidden_dim1 = hidden_size\n",
        "        self.hidden_dim2 = hidden_size2\n",
        "        self.DAN_depth = DAN_depth\n",
        "\n",
        "        self.mc = nn.ModuleList([nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "        self.mp = nn.ModuleList([nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "\n",
        "        self.vc = nn.ModuleList([nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "        self.rp = nn.ModuleList([nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "        self.vs = nn.ModuleList([nn.Linear(self.hidden_dim2, 1) for i in range(self.DAN_depth)]).cuda()\n",
        "        self.rs = nn.ModuleList([nn.Linear(self.hidden_dim2, 1) for i in range(self.DAN_depth)]).cuda()\n",
        "\n",
        "        self.v_to_r_transform = nn.ModuleList(\n",
        "            [nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "        self.r_to_v_transform = nn.ModuleList(\n",
        "            [nn.Linear(self.hidden_dim2, self.hidden_dim2) for i in range(self.DAN_depth)]).cuda()\n",
        "\n",
        "        self.GRU_dan = nn.GRUCell(self.hidden_dim2, self.hidden_dim2).cuda()\n",
        "        self.W_out = nn.Linear(self.hidden_dim2 * self.hidden_dim2 * 2, 1).cuda()\n",
        "\n",
        "    def mask_softmax(self, a, mask, dim=-1):\n",
        "        a_max = torch.max(a, dim, keepdim=True)[0]\n",
        "        a_exp = torch.exp(a - a_max)\n",
        "        a_exp = a_exp * mask\n",
        "        a_softmax = a_exp / (torch.sum(a_exp, dim, keepdim=True) + 1e-6)\n",
        "        return a_softmax\n",
        "\n",
        "    def dan_gru(self, batch_size, comp_feature, prot_feature, vertex_mask, seq_mask, pairwise_pred):\n",
        "        vertex_mask = vertex_mask.view(batch_size, -1, 1)\n",
        "        seq_mask = seq_mask.view(batch_size, -1, 1)\n",
        "\n",
        "        c0 = torch.sum(comp_feature * vertex_mask, dim=1) / torch.sum(vertex_mask, dim=1)  # batch x hidden2\n",
        "        p0 = torch.sum(prot_feature * seq_mask, dim=1) / torch.sum(seq_mask, dim=1)  # batch x hidden2\n",
        "\n",
        "        m = c0 * p0  # batch x hidden2\n",
        "        for DAN_iter in range(self.DAN_depth):\n",
        "            r_to_v = torch.matmul(pairwise_pred, F.tanh(self.r_to_v_transform[DAN_iter](prot_feature)))  # batch x atom 개수 x hidden2, equ23\n",
        "            v_to_r = torch.matmul(pairwise_pred.transpose(1, 2), F.tanh(self.v_to_r_transform[DAN_iter](comp_feature)))  # batch x residue 개수 x hidden2, equ24\n",
        "\n",
        "            v_tmp = F.tanh(self.vc[DAN_iter](comp_feature)) * F.tanh(self.mc[DAN_iter](m)).view(batch_size, 1,-1) * r_to_v  # batch x atom 개수 x hidden2, equ25\n",
        "            r_tmp = F.tanh(self.rp[DAN_iter](prot_feature)) * F.tanh(self.mp[DAN_iter](m)).view(batch_size, 1,-1) * v_to_r  # batch x residue 개수 x hidden2, equ26\n",
        "\n",
        "            v_att = self.mask_softmax(self.vs[DAN_iter](v_tmp).view(batch_size, -1),vertex_mask.view(batch_size, -1))  # batch x atom개수, equ27\n",
        "            r_att = self.mask_softmax(self.rs[DAN_iter](r_tmp).view(batch_size, -1),seq_mask.view(batch_size, -1))  # batch x residue 개수, equ28\n",
        "\n",
        "            compound_fixed = torch.sum(comp_feature * v_att.view(batch_size, -1, 1), dim=1)  # batch x hidden2, equ29\n",
        "            protein_fixed = torch.sum(prot_feature * r_att.view(batch_size, -1, 1), dim=1)  # batch x hidden2, equ30\n",
        "\n",
        "            m = self.GRU_dan(m, compound_fixed * protein_fixed)  # batch x hidden2, equ31\n",
        "\n",
        "        return compound_fixed, protein_fixed, m\n",
        "\n",
        "    def forward(self, comp_feature, super_feature, prot_feature, vertex_mask, seq_mask, pairwise_pred):\n",
        "        batch_size = comp_feature.size(0)\n",
        "\n",
        "        weight_atom = initialize_(batch_size, self.hidden_dim1, self.hidden_dim2)\n",
        "        weight_super = initialize_(batch_size, self.hidden_dim1, self.hidden_dim2)\n",
        "        weight_protein = initialize_(batch_size, self.hidden_dim1, self.hidden_dim2)\n",
        "\n",
        "        comp_feature = embedding_(comp_feature, weight_atom)  # batch x atom 개수 x hidden2\n",
        "        super_feature = embedding_(super_feature, weight_super)  # batch x 1 x hidden2\n",
        "        prot_feature = embedding_(prot_feature, weight_protein)  # batch x residue 개수 x hidden2\n",
        "\n",
        "        cf, pf, m = self.dan_gru(batch_size, comp_feature, prot_feature, vertex_mask, seq_mask,pairwise_pred)\n",
        "        combined_representation = torch.cat([cf.view(batch_size, -1), super_feature.view(batch_size, -1)],dim=1)  # batch x 2*hidden2\n",
        "        flatten = F.leaky_relu(torch.matmul(combined_representation.view(batch_size, -1, 1), pf.view(batch_size, 1, -1)).view(batch_size, -1), 0.1)  # batch x 2 * hidden2^2\n",
        "        affinity_pred = self.W_out(flatten)  # batch x 1\n",
        "\n",
        "        return affinity_pred, pairwise_pred\n"
      ],
      "metadata": {
        "id": "HXMm2Mt5usk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network(GNN_depth=4, k_head=3, hidden_dim1=128, hidden_dim2=128, DAN_depth = 2).cuda()\n",
        "net.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZJFmOlH6b9E",
        "outputId": "1ea27a5b-3c58-479a-8723-e9e34485ab33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7fe181fbd650>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Masked_BCELoss(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Masked_BCELoss, self).__init__()\n",
        "\t\tself.criterion = nn.BCELoss(reduce=False)\n",
        "\tdef forward(self, pred, label, pairwise_mask, vertex_mask, seq_mask):\n",
        "\t\tbatch_size = pred.size(0)\n",
        "\t\tloss_all = self.criterion(pred, label)\n",
        "\t\tloss_mask = torch.matmul(vertex_mask.view(batch_size,-1,1), seq_mask.view(batch_size,1,-1))*pairwise_mask\n",
        "\t\tloss = torch.sum(loss_all*loss_mask) / torch.sum(pairwise_mask).clamp(min=1e-10)\n",
        "\t\treturn loss"
      ],
      "metadata": {
        "id": "2-BhsTRvIQlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1].view(8,-1,1).shape, data[7].view(8,1,-1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQAU4xh0JgC8",
        "outputId": "d6a1c11f-2301-4afe-89df-73b95035e53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 37, 1]), torch.Size([8, 1, 844]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[8].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXcS317Lp9SP",
        "outputId": "78753642-cc71-4cc1-9b88-bc2271aaa096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(51.3000, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_all.view(-1,1,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJB50xqKK6zY",
        "outputId": "35120ab0-22e3-4bad-a4f1-0f90582d764e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([249824, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByhxThdBJ8GJ",
        "outputId": "611eb44b-83f2-41d3-b290-813cd172599e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 37, 844])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[8].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5sSwdAoI6PA",
        "outputId": "380f3a53-34a6-4c31-a849-efe15d2089d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bce = nn.BCELoss(reduce=False)\n",
        "loss_all = bce(pairwise,data[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyBSpL_QJIis",
        "outputId": "cc4b5e7f-aeb3-4c35-9018-b4d2a6dbc7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI7Gz2YAKjFU",
        "outputId": "f583f210-2a53-4ade-b130-4159935fef73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 37, 844])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion2 = Masked_BCELoss()\n",
        "loss_pairwise = criterion2(pairwise, data[9], data[10], data[1], data[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otjUrnhgIYkV",
        "outputId": "f8dd65f7-103a-4b3f-847d-db295a32adbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_pairwise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBTPxSKWLp_2",
        "outputId": "9a834e09-e6bc-4156-9c9f-530f80fa3851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "def train(train_loader, valid_loader, test_loader, epochs, lambd):\n",
        "\n",
        "  net = Network(GNN_depth=4, k_head=3, hidden_dim1=128, hidden_dim2=128, DAN_depth = 2).cuda()\n",
        "\n",
        "  criterion1 = nn.MSELoss()\n",
        "  criterion2 = Masked_BCELoss()\n",
        "\n",
        "  optimizer = optim.Adam(net.parameters(), lr=0.0005, weight_decay=0, amsgrad=True)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "  min_rmse=1000\n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch {} started!'.format(epoch))\n",
        "\n",
        "    train_output_list = []\n",
        "    train_label_list = []\n",
        "    total_loss = 0\n",
        "    affinity_loss = 0\n",
        "    pairwise_loss = 0\n",
        "    for i, train_data in enumerate(train_loader):\n",
        "      batch_size = train_data[0].size(0)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      affinity_pred, pairwise_pred = net(train_data[0], train_data[1], train_data[2], train_data[3], train_data[4], train_data[5], train_data[6], train_data[7])\n",
        "\n",
        "      loss_aff = criterion1(affinity_pred, train_data[8])\n",
        "      loss_pairwise = criterion2(pairwise_pred,train_data[9], train_data[10], train_data[1], train_data[7])\n",
        "      loss = loss_aff + lambd*loss_pairwise\n",
        "\n",
        "      total_loss += float(loss.data*batch_size)\n",
        "      affinity_loss += float(loss_aff.data*batch_size)\n",
        "      pairwise_loss += float(loss_pairwise.data*batch_size)\n",
        "\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
        "      optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    loss_list = [total_loss, affinity_loss, pairwise_loss]\n",
        "    loss_name = ['total loss', 'affinity loss', 'pairwise loss']\n",
        "    print_loss = [loss_name[i]+' '+str(round(loss_list[i]/float(len(train_data[0])), 6)) for i in range(len(loss_name))]\n",
        "    print ('epoch:',epoch, ' '.join(print_loss))\n",
        "\n",
        "    valid_rmse, valid_auc, valid_label, valid_output = test(net, valid_loader)\n",
        "    print('Valid rmse : {}\\n'.format(valid_rmse) + 'Valid average AUC : {}'.format(valid_auc))\n",
        "    print('Valid rmse : {}\\n'.format(valid_rmse))\n",
        "\n",
        "    if valid_rmse < min_rmse:\n",
        "    #if valid_auc > max_auc:\n",
        "      min_rmse = valid_rmse\n",
        "      #max_auc = valid_auc\n",
        "      test_rmse, test_auc, test_label, test_output = test(net, test_loader)\n",
        "      print('Test rmse : {}\\n'.format(test_rmse) + 'Test average AUC : {}'.format(test_auc))\n",
        "      print('Test rmse : {}\\n'.format(test_rmse))\n",
        "\n",
        "  print('Train finished!')\n",
        "  return test_rmse, test_auc, test_label, test_output\n",
        "\n",
        "\n",
        "def test(net, test_loader):\n",
        "  output_list = []\n",
        "  label_list = []\n",
        "  pairwise_auc_list = []\n",
        "\n",
        "  for i, test_data in enumerate(test_loader):\n",
        "    batch_size = test_data[0].size(0)\n",
        "\n",
        "    affinity_pred, pairwise_pred = net(test_data[0], test_data[1], test_data[2], test_data[3], test_data[4], test_data[5], test_data[6], test_data[7])\n",
        "\n",
        "    # for j in range(len(test_data[10])):\n",
        "    #   num_vertex = int(torch.sum(test_data[1][j,:]))\n",
        "    #   num_residue = int(torch.sum(test_data[7][j,:]))\n",
        "    #   pairwise_pred_i = pairwise_pred[j, :num_vertex, :num_residue].cpu().detach().numpy().reshape(-1)\n",
        "    #   pairwise_label_i = test_data[10][j,:num_vertex,:num_residue].cpu().detach().numpy().reshape(-1)\n",
        "    #   pairwise_auc_list.append(roc_auc_score(pairwise_label_i, pairwise_pred_i))\n",
        "    output_list += affinity_pred.cpu().detach().numpy().reshape(-1).tolist()\n",
        "    label_list += test_data[8].reshape(-1).tolist()\n",
        "  output_list = np.array(output_list)\n",
        "  label_list = np.array(label_list)\n",
        "  rmse_value = np.sqrt(mean_squared_error(label_list,output_list))\n",
        "  #average_pairwise_auc = np.mean(pairwise_auc_list)\n",
        "\n",
        "  return rmse_value, 1, label_list, output_list\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "G2ufOWqhBfjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rmse, test_auc, test_label, test_output = train(train_loader, valid_loader, test_loader, 3,0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "Jms1dTr3AWi0",
        "outputId": "3ba7bcf6-3a54-4911-8c95-d7fc1606c489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 started!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 total loss 3442.2249 affinity loss 3442.2249 pairwise loss 0.0\n",
            "Valid rmse : 2.5780447802050883\n",
            "Valid average AUC : 1\n",
            "Valid rmse : 2.5780447802050883\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-375f6fc0b27a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-257-7d42be726e6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, test_loader, epochs, lambd)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mmin_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_rmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;31m#max_auc = valid_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mtest_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test rmse : {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rmse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Test average AUC : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test rmse : {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-257-7d42be726e6f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, test_loader)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mpairwise_auc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-206-2d37f4a56764>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_and_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_bondwise_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack_and_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_atom_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-204-947b2818a35d>\u001b[0m in \u001b[0;36mstack_and_pad\u001b[0;34m(arr_list, max_length)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'null'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[7].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN2Bd-1SRpF2",
        "outputId": "bdfcc158-14ae-46b4-85f0-2f26ed17e21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 844])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[9].shape, data[10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaPtjTs4Hhns",
        "outputId": "3c22f81b-e118-48cc-efc8-266a7f25f96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 67, 346]), torch.Size([8, 67, 346]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network(GNN_depth=4, k_head=3, hidden_dim1=128, hidden_dim2=128, DAN_depth = 2).cuda()\n",
        "affinity, pairwise = net(data[0], data[1], data[2], data[3], data[4], data[5], data[6], data[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdLoGdqJvXqs",
        "outputId": "d4c5d79a-8c85-447e-c7bd-f915880b86b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "affinity, pairwise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzTlDA2TEo0W",
        "outputId": "07b4706e-3033-49ac-e1a6-3303c2e9de55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0694],\n",
              "         [-0.1045],\n",
              "         [-0.0691],\n",
              "         [-0.0164],\n",
              "         [ 0.0290],\n",
              "         [ 0.0617],\n",
              "         [-0.0709],\n",
              "         [-0.0851]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
              " tensor([[[0.5361, 0.5373, 0.5494,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5584, 0.5549, 0.5721,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5487, 0.5471, 0.5608,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.5591, 0.5638, 0.5643,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5770, 0.5822, 0.5860,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5554, 0.5603, 0.5650,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.5755, 0.5807, 0.5818,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5735, 0.5787, 0.5794,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5777, 0.5830, 0.5849,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.5362, 0.5408, 0.5374,  ..., 0.5311, 0.5346, 0.5284],\n",
              "          [0.5689, 0.5715, 0.5831,  ..., 0.5708, 0.5667, 0.5629],\n",
              "          [0.5548, 0.5606, 0.5686,  ..., 0.5561, 0.5548, 0.5499],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0.5335, 0.5337, 0.5354,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5504, 0.5503, 0.5454,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5822, 0.5849, 0.5821,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.5557, 0.5462, 0.5771,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5805, 0.5702, 0.6190,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5562, 0.5470, 0.5781,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.5394, 0.5430, 0.5444,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5634, 0.5689, 0.5655,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.5394, 0.5430, 0.5444,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "        device='cuda:0', grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c = stack_and_pad(pairwise.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "4NBfPP1uEwtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " gru = nn.GRU(82,82).cuda()\n",
        "output , h1 = gru(data[0])"
      ],
      "metadata": {
        "id": "50J6AGtB6GMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[6].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oavTh9N7wbij",
        "outputId": "3f61f0a7-b4d4-4ffa-cecf-e15f3b829f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 844, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6hSYp_s-CTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}